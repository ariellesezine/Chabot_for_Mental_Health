{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1aa192",
   "metadata": {},
   "source": [
    "# CHATBOT INTELLIGENT USING NLP et machine learning\n",
    "\n",
    "## BY SEZINE ARIELLE\n",
    "\n",
    "#### La santé mentale fait référence à l'état de bien-être émotionnel, psychologique et social d'une personne. Elle influence la manière dont une personne pense, ressent, et agit, ainsi que sa capacité à gérer le stress, à nouer des relations et à prendre des décisions. La santé mentale est une composante essentielle de la santé globale et affecte tous les aspects de la vie d'une personne.\n",
    "\n",
    "Développer un chatbot pour ***la santé mentale*** est crucial car il offre un accès immédiat et discret à un soutien psychologique, particulièrement pour les personnes qui peuvent hésiter à consulter un professionnel en raison de la stigmatisation ou des contraintes d'accessibilité. Les chatbots peuvent fournir des conseils, des ressources, et une écoute active 24/7, aidant ainsi à prévenir les crises en intervenant tôt. Ils permettent aussi de décharger les professionnels en offrant une première ligne de soutien, rendant les soins de santé mentale plus accessibles à un large public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd9ddca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "![sante_mentale]('mental_health.jfif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2104f",
   "metadata": {},
   "source": [
    "# Analyse exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b186d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des bibliotheques necéssaires pour la manipulation des données\n",
    "import pandas as pd\n",
    "import json\n",
    "#chargement des bibliotheques necessaires pour le nlp\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Télécharger les ressources \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4de1f27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#chargement des données\n",
    "with open('KB.json','rb') as file:\n",
    "    data=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89a898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### on transforme notre fichier json en dataframme pour le rendre plus lisible\n",
    "df=pd.DataFrame(data['intents'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5ec96",
   "metadata": {},
   "source": [
    "la cellule ci haut nous permet de voir qu'il ya une entité dictionnaire ou on a mal nomée le champ responses on va donc le renomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a976f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recherche de l'index ou s'est produit l'erreur\n",
    "for i,elt in enumerate(df['response']):\n",
    "        print(f'{i}------------{elt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608ddb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on se rassure que dans cha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9264f967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"responses\"][23]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188014b",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fad78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ayant remarquer que cette erreur s'est produite a la l'index 23, on la corrige\n",
    "df[\"responses\"][23]=df[\"response\"][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4898791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"responses\"][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2616f190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ad3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime la colonne response qui ne sera par suite d'aucune utilité\n",
    "df=df.drop(columns=['response'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1a373",
   "metadata": {},
   "source": [
    "Afin de créer nos données d’entraînement, nous devons d’abord effectuer certaines opérations sur nos données telles que :\n",
    "\n",
    "- Créer un vocabulaire de tous les mots utilisés dans les patterns \n",
    "\n",
    "- Créer une liste des classes – Il s’agit simplement des tags de chaque intention.\n",
    "\n",
    "- Créer une liste de tous les patterns dans le fichier des intentions.\n",
    "\n",
    "- Créer une liste de touts les tags associés à chaque pattern dans le fichier intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4210dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=[]\n",
    "vocabulary=[]\n",
    "patterns=[]\n",
    "tags=[]\n",
    "\n",
    "for intent in data[\"intents\"]: \n",
    "    for pattern in intent['patterns']:\n",
    "        tokens=word_tokenize(pattern.lower())\n",
    "        for mot in tokens:\n",
    "            if mot not in vocabulary:\n",
    "                vocabulary.append(mot)\n",
    "                \n",
    "    patterns.extend(intent['patterns'])\n",
    "\n",
    "    tags.append(intent['tag'])\n",
    "\n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa72952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9fac644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2167b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary=[mot for mot in vocabulary if mot not in string.punctuation]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8be9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3f2af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary=[mot for mot in vocabulary if mot not in stop_words]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cc092",
   "metadata": {},
   "source": [
    "### lemmatisation des mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2125893",
   "metadata": {},
   "source": [
    "La lemmatisation est le processus de transformation d'un mot sous sa forme de base ou de \"lemme\", qui est la forme canonique ou le dictionnaire d'un mot. Contrairement à la racinisation (stemming), qui coupe les suffixes pour réduire un mot à sa racine, la lemmatisation utilise des règles linguistiques et des bases de données lexicales pour transformer un mot en une forme valide grammaticalement.En utilisant les lemmes, les algorithmes de NLP peuvent mieux comprendre le contexte d'un texte, ce qui est essentiel pour des tâches comme la classification de texte\n",
    "\n",
    "Exemples :\n",
    "\n",
    "- \"running\" devient \"run\"\n",
    "- \"better\" devient \"good\"\n",
    "- \"geese\" devient \"goose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1370d5e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Télécharger les ressources nécessaires pour lemmatizer (si ce n'est pas déjà fait)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "740d56c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialiser le lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Appliquer le lemmatizer\n",
    "vocabulary_final = [lemmatizer.lemmatize(word) for word in vocabulary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa86ac",
   "metadata": {},
   "source": [
    "### Encodage de la target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446f32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialiser le Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir les étiquettes catégorielles en valeurs numériques\n",
    "labels_numeriques = label_encoder.fit_transform(classes)\n",
    "print(labels_numeriques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3e2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"tag\":[], \"patterns\":[], \"responses\":[]}\n",
    "for i in range(len(df)):\n",
    "    ptrns = df[df.index == i]['patterns'].values[0]\n",
    "    rspns = df[df.index == i]['responses'].values[0]\n",
    "    tag = df[df.index == i]['tag'].values[0]\n",
    "    for j in range(len(ptrns)):\n",
    "        dic['tag'].append(tag)\n",
    "        dic['patterns'].append(ptrns[j])\n",
    "        dic['responses'].append(rspns)\n",
    "        \n",
    "df = pd.DataFrame.from_dict(dic)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30427aa2",
   "metadata": {},
   "source": [
    "#### on doit maintenant mettre les patterns dans un format adequat en:\n",
    "- rendant tout en miniscule\n",
    "- tokenisant le texte\n",
    "- supprimant les stopwords\n",
    "- lemmatisant le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e55c0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2a9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['patterns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45201018",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_patterns=[]\n",
    "def preprocessing_patterns(df):\n",
    "    for i in range (df.shape[0]):\n",
    "        texte=df['patterns'][i]\n",
    "        texte=texte.lower()\n",
    "        tokens=word_tokenize(texte)\n",
    "        tokens_no_punk=[mot for mot in tokens if mot not in stop_words]\n",
    "        tokens_lem=[lemmatizer.lemmatize(mot) for mot in tokens_no_punk]\n",
    "        tokens_char=' '.join(tokens_lem)\n",
    "        cleaned_patterns.append(tokens_char)\n",
    "        return cleaned_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ab8210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_patterns=preprocessing_patterns(df)\n",
    "cleaned_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067362e7",
   "metadata": {},
   "source": [
    "###  on commencera par subdiviser notre ensembe de donné en X et y:\n",
    " - X pour les patterns ou messages que nos utilisateurs peuvent taper,\n",
    " - y pour le tag, categorie a laquelle appartient le pattern ou message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a282826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25521632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29733a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff787c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7181c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018d825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea89d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c3265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f25903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc53962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba990106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb418ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253357a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac17bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9348201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff3446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f291dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffb07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2b068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f842e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c5db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f448ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83bbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac8ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3b126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba805d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4328a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb110d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
